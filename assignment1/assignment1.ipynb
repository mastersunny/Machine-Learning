{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo.png\" style=\"width: 100px;\"/>\n",
    "<h1><center>Assignment 1</center></h1>\n",
    "<h3><center>Basic Concepts of Machine Learning and Foundations of Concept Learning</h3></center>\n",
    "\n",
    "<center>Due: 02.11.2021 at 23:59</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1) Hypothesis Testing\n",
    "\n",
    "<br>\n",
    "Consider the mushroom dataset at \"data/UCI_mushroom_data.csv\". There are a lot of free datasets available at the UCI repository for machine learning practitioners to develop new machine learning approaches.<br>\n",
    "The mushroom dataset contains information about different kinds of mushrooms.<br> <br>\n",
    "There is a description about the different attributes and the dataset's origin in the \"data/UCI_mushroom_data_description.txt\" file. <br>\n",
    "\n",
    "<br> <br>\n",
    "The first column of the dataset contains the class labels: <br>\n",
    "\n",
    "- **1** denotes a poisonous mushroom\n",
    "- **0** denotes an eatable mushroom\n",
    "\n",
    "There are **2156** poisonous and **3488** eatable mushrooms in the dataset.\n",
    "<br><br>\n",
    "First implement a function **is_more_general** which takes two hypotheses and checks whether the first hypothesis is more general than the second one. An example hypothesis to **detect poisonous mushrooms** is given below.\n",
    "<br><br>\n",
    "Your next task is to implement the FIND-S algorithm on the mushroom dataset to find a maximally specific hypothesis which is consistent with the mushroom dataset to **detect poisonous mushrooms**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, first start by loading and inspecting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hypotheses should be in the form of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = ['?','?','?','?','f','f','c','b','?','e','?','?','?','?','?','p','?','?','?','h','v','?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the function **is_more_general**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function is_more_general should take two hypotheses and check\n",
    "# whether the first of them is more general than the second.\n",
    "# If the first is more general it should return True, otherwise False.\n",
    "# Be aware: Sometimes one can not order hypotheses in a \"more general than\" relation.\n",
    "# In this case the function should also return False.\n",
    "\n",
    "def is_more_general(first_hypothesis, second_hypothesis): # given\n",
    "    # TODO\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example test cases (you can define your own as well, there are more possibilities!)\n",
    "\n",
    "hypothesis1 = ['?','?','?','f','?','f','?','b','?','e','b','?','?','?','?','p','?','?','?','h','v','?']\n",
    "hypothesis2 = ['?','?','?','f','f','?','c','b','?','e','?','?','?','?','?','?','?','?','?','h','v','?']\n",
    "\n",
    "\n",
    "#print(is_more_general(hypothesis1, hypothesis2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement the FIND-S algorithm presented in the lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_s(attributes, target): # given\n",
    "    # TODO\n",
    "    return hypothesis # given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And test your find_s function on the mushroom dataset\n",
    "\n",
    "hypothesis = find_s(attributes, target)\n",
    "hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**QUESTION**:   \n",
    "Do you notice anything about the resulting hypothesis?   \n",
    "Is FIND-S a suitable way for finding a hypothesis on eating mushrooms? Justify your answer.\n",
    "\n",
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply the hypothesis you found above using the find_s function on the mushroom dataset. Save the predictions in an array.  \n",
    "If you didn't find a hypothesis use this one:\n",
    "> h_test=['?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','p','?','?','?','?','?','?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Prediction with the hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Evaluation Metrics\n",
    "Now that we know how to generate a hypothesis for concept learning, we want to be able to tell if the hypothesis is actually good or not. Therefore, we have to evaluate what prediction our hypothesis makes on a set of instances and compare these predictions to the instances' target concept.\n",
    "\n",
    "For this task, let us assume the following given set of target concepts and according predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target_concept = np.loadtxt('data/target_concepts.csv').astype(int)\n",
    "predictions = np.loadtxt('data/predictions.csv').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already learned some metrics for hypotheses in the tutorial. We now want to apply these metrics to the data we just loaded. Start of by calculating the number of **True Positives (TP)**, **False Positives (FP)**, **True Negatives (TN)**, and **False Negatives (FN)**. Store the values in the given variables, you are going to use them later. Use the code given below to display the numbers in a **confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0 # insert your formula here\n",
    "tn = 0 # insert your formula here\n",
    "fp = 0 # insert your formula here\n",
    "fn = 0 # insert your formula here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "print(\"{:<19}{:^18}\".format('', 'Ground Truth'))\n",
    "print(\"{:<10}|{:^9}|{:^9}|{:^9}|\".format('', '', 'Positive', 'Negative'))\n",
    "print(\"{:<10}|{:<9}|{:^9}|{:^9}|\".format('Prediction', 'Positive', tp, fp))\n",
    "print(\"{:<10}|{:<9}|{:^9}|{:^9}|\".format('', 'Negative', fn, tn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculate the **accuracy**, using the variables from before. Use the formula you learned in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy reduces the entire confusion matrixy to one single metric. However, this metric does not distinguish if the false predictions are false positives or false negatives. If the use case of our hypothesis requires us to make this distinction, we need other metrics. In the tutorial, you already heard about two of those metrics , namely **recall** and **precision**. Calculate both of these for the given dataset, again with the formula from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is more important -- recall or precision? That depends on how you want to use the hypothesis. Consider the following two use cases. For each of them, think about what error is more tolerable (false positives or false negatives), and thereby if recall or precision  is more important.\n",
    "\n",
    "#### Use Case 1:\n",
    "> Look at the use case from the first task. Your hypothesis is used to detect poisonous mushrooms. Imagine that those mushrooms that the hypothesis determines to be poisonous (*positive*) will be thrown away, but those that the hypothesis finds to be eatable (*negative*) will be mixed into your dinner soup.\n",
    "\n",
    "Are false positives or false negatives more tolerable? Thereby, is recall or precision more important?\n",
    "\n",
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Case 2:\n",
    "> You work in a factory that manufactures products with very high quality requirements. The goal is that you must not ship a product that is flawed in any way. You are asked to build a system that supports the quality assurance process. The new process should look like this: Each manufactured product will first be evaluated by your system. If the system finds that the product fulfills all the quality standards (*positive*), it will automatically be shipped. If the system finds any flaws with the product (*negative*), it will be manually inspected by a quality specialist.\n",
    "\n",
    "Are false positives or false negatives more tolerable? Thereby, is recall or precision more important?\n",
    "\n",
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, in many use cases we want to prevent both false negatives and false positives, so we have to consider both recall and precision. In the tutorial, you learned how to combine them into the **F1-score**. Calculate this for the dataset with the formula you learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute, didn't we already have a metric to reduce the whole confusion matrix into one number? We did, it was the accuracy! So why do we even need the F1-score? What does it take into account that the accuracy doesn't?\n",
    "\n",
    "*Your solution here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we know how to calculate some metrics for the quality of our hypothesis, and we know how to interpret them. Usually, when we evaluate a model, we do not want to bother ourselves with doing the calculations by hand. Instead, we use library functions for that.\n",
    "\n",
    "An often used library for machine learning is [scikit-learn](https://scikit-learn.org/stable/modules/classes.html). It contains easy-to-use implementations of a variety of machine learning tasks - including evaluation. You can expect to see it a few times throughout the semester, and we are going to use it for this last task.\n",
    "\n",
    "Now, calculate alle the metrics from before (confusion matrix, accuracy, recall, precision, F1-score) again, this time using the functions from `scikit-learn`. Check if the outputs match the outputs of your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tips and tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1) Embedding images\n",
    "You can embed images in a jupyter notebook on two ways: <br/>\n",
    "**First**, you can use the IPython kernel to draw an image everytime the code cell is run like shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"images/logo.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, you can embed images directly in a Markdown cell as shown below. You can either use markdown syntax or write plain HTML code. Sometimes HTML code is more practical, as you have much finer control over the HTML elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Markdown syntax:\n",
    "```\n",
    "![title](images/logo.png)\n",
    "```\n",
    "2. HTML syntax\n",
    "```\n",
    "<img src=\"images/logo.png\" style=\"width: 70px;\"/>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "notebook_metadata_filter": "-all",
   "text_representation": {
    "extension": ".py",
    "format_name": "light"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
