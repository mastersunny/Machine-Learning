{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ced5bfb",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1faba108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "#target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('training_data.csv', delimiter=',')\n",
    "\n",
    "tweet_tokens = []\n",
    "target = []\n",
    "\n",
    "positive_row_count=0\n",
    "negative_row_count=0\n",
    "for row in df.values:\n",
    "    if row[0]==0 and negative_row_count<50000:\n",
    "        negative_row_count+=1\n",
    "        tweet_tokens.append(word_tokenize(row[5]))\n",
    "        target.append(row[0])\n",
    "    elif row[0]==4 and positive_row_count<50000:\n",
    "        positive_row_count+=1\n",
    "        tweet_tokens.append(word_tokenize(row[5]))\n",
    "        target.append(row[0])\n",
    "\n",
    "print(len(tweet_tokens))\n",
    "print(len(target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dc5d92",
   "metadata": {},
   "source": [
    "# Removing Noise from the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3e022bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "    \n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words and token not in [\"...\",\"http\"]:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178b2518",
   "metadata": {},
   "source": [
    "# Clean the Positive and Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f8321250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'switchfoot', 'http', ':', '//twitpic.com/2y1zl', '-', 'Awww', ',', 'that', \"'s\", 'a', 'bummer', '.', 'You', 'shoulda', 'got', 'David', 'Carr', 'of', 'Third', 'Day', 'to', 'do', 'it', '.', ';', 'D']\n",
      "['switchfoot', '//twitpic.com/2y1zl', 'awww', \"'s\", 'bummer', 'shoulda', 'get', 'david', 'carr', 'third', 'day']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cleaned_tokens_list = []\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "for tokens in tweet_tokens:\n",
    "    cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "    \n",
    "print(tweet_tokens[0])\n",
    "print(cleaned_tokens_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92d278",
   "metadata": {},
   "source": [
    "# Determining Word Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d1743826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd8b30c",
   "metadata": {},
   "source": [
    "# Testing Word Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c63c0415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'s\", 10848), (\"n't\", 10847), ('get', 10464), ('go', 9948), (\"'m\", 7822), ('good', 7218), ('day', 6551), ('work', 6102), ('..', 5871), ('like', 5030)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6a98dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, lowercase=False)\n",
    "all_features = tfidf.fit_transform(cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d6cb48fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<100000x91848 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 783341 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d1d51e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_features, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "37c5e1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80000, 91848)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "748f2854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_nb_classifier = MultinomialNB()\n",
    "multinomial_nb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5a0e829e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.80      0.76     10035\n",
      "           4       0.78      0.70      0.74      9965\n",
      "\n",
      "    accuracy                           0.75     20000\n",
      "   macro avg       0.75      0.75      0.75     20000\n",
      "weighted avg       0.75      0.75      0.75     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = multinomial_nb_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "63415ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tweet = \"I am so happy\"\n",
    "\n",
    "tokens = word_tokenize(custom_tweet)\n",
    "custom_tweet_tokens = []\n",
    "custom_tweet_tokens.append(remove_noise(tokens, stop_words))\n",
    "features =  tfidf.transform(custom_tweet_tokens)\n",
    "\n",
    "multinomial_nb_classifier.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b3b2c457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svc_classifier = LinearSVC()\n",
    "linear_svc_classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "3a2934cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75     10035\n",
      "           4       0.75      0.77      0.76      9965\n",
      "\n",
      "    accuracy                           0.76     20000\n",
      "   macro avg       0.76      0.76      0.75     20000\n",
      "weighted avg       0.76      0.76      0.75     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = linear_svc_classifier.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "10e9c5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tweet = \"Things are about to change for you. May the overthinking, and the doubt exit your mind right now. May clarity replace confusion. May peace and calmness fill your life. You’ve been strong long enough, it’s time to start receiving your blessings. You deserve it.\"\n",
    "\n",
    "tokens = word_tokenize(custom_tweet)\n",
    "custom_tweet_tokens = []\n",
    "custom_tweet_tokens.append(remove_noise(tokens, stop_words))\n",
    "features =  tfidf.transform(custom_tweet_tokens)\n",
    "\n",
    "linear_svc_classifier.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb69b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
